{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 1 - NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revision de ___ en comentarios de libros "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#Librerias para sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "\n",
    "#Librerias para vectorizacion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd. read_csv('data/reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparación e los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Se se paro la columna helpfull en dpos columnas. Las veces que se le dio helpfull postivo, y las veces totales que se le ha dado helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['Helpful Total'] = data['helpful'].str.replace(\"[\",\"\")\n",
    "data['Helpful Total'] = data['Helpful Total'].str.replace(\"]\",\"\")\n",
    "separacion = data['Helpful Total'].str.split(\",\",expand=True)\n",
    "data['Helpful Total']=separacion[1].astype(int)\n",
    "data['Helpful Positivo']=separacion[0].astype(int)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Se omiteron los regitso que no han sido calificados helpfull ninguna vezy se calculo la tasa de positividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filtar los reviws que no han sido clasificados como helpful\n",
    "data = data[data[\"Helpful Total\"] >0 ]\n",
    "data['Helpful Rate'] = data['Helpful Positivo']/data['Helpful Total']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Distribución de la tasa de postividad en helpfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data[\"Helpful Rate\"],bins = 10).set(title=\"Distribución del helpful rate\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición  de la vriable de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Output\"] = np.where(data[\"Helpful Rate\"] >= 0.9,1, 0)\n",
    "sns.countplot(x=\"Output\", data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = data[[\"Output\", \"reviewText\"]]\n",
    "df = df.reset_index()\n",
    "df = df.drop(columns=[\"index\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import emoji\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import en_core_web_sm\n",
    "import unidecode\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puntucacion\n",
    "punctuations = string.punctuation\n",
    "# Carga los modelos de spacy y las stopwords\n",
    "parser = en_core_web_sm.load()\n",
    "stop_words = STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Elimina espacios, emojis, números y convierte todo a minúscula.\n",
    "    text = str(text)\n",
    "    text = text.replace('\\n','').replace('\\r','').replace('\\t','').strip().lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    allchars = [str for str in text] \n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    text=''.join((c for c in unicodedata.normalize('NFD',text) if unicodedata.category(c) != 'Mn'))\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "    # Tokenize\n",
    "    token = parser(text)\n",
    "    # Lemmatize\n",
    "    token = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in token]\n",
    "    # Quita stopwords y signos de puntuación\n",
    "    token = ' '.join([word for word in token if word not in stop_words and word not in punctuations])\n",
    "    token = unidecode.unidecode(token)\n",
    "    token = ' '.join([w for w in token.split() if len(w)>1])\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"CleanText\"]=df[\"reviewText\"].apply(clean_text)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.drop(columns=\"reviewText\")\n",
    "df.to_csv(\"dataNLP.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "positive_text = ''.join(df[df['Output'] == 1].CleanText)\n",
    "word_cloud = WordCloud(stopwords = STOPWORDS, max_words = 100, width=1366, height=768, background_color=\"white\").generate(positive_text)\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word cloud de comentarios postivos',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "negative_text = ''.join(str(df[df['Output'] == 0].CleanText))\n",
    "word_cloud = WordCloud(stopwords = STOPWORDS, max_words = 100, width=1366, height=768, background_color=\"white\").generate(negative_text)\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word cloud de comentarios negativos',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversión a vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importación del nuevo dataset. Solo se toman  50000 registros porque si no es muy pesado para crear lasm atrices y ajustar el modelo.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv('dataNLP.csv', index_col=0)\n",
    "df=df.dropna()\n",
    "df=df.iloc[0:50000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se separa la muestra 90% train y 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df[\"Output\"]\n",
    "X = df[\"CleanText\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se codifican las matrices bajo el algoritmo TF_IDF. Solo se incluyen las ppalabras que están más de 50 veces en el dataframe para evitar palabras no significativas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df = 50)\n",
    "\n",
    "x_train_vect = tfidf_vectorizer.fit_transform(x_train).toarray()\n",
    "x_test_vect = tfidf_vectorizer.transform(x_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 4280)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_vect.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo Random Forest con represntación TF - IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un Pipeline con un modelo RanfomForest que se calibra por medio de  RandomSearchCV para encontrar los hiperparámetros que minimizan el error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline_RF = Pipeline(steps = [(\"RF\", RandomForestClassifier())])\n",
    "\n",
    "Param_RF = {\n",
    "    'RF__max_depth': np.linspace(1, 20, 20, endpoint=True),\n",
    "    'RF__max_features': ['auto','log2','sqrt',None],\n",
    "    'RF__min_samples_split':np.linspace(0.1, 1.0, 10, endpoint=True),\n",
    "    'RF__min_samples_leaf':np.linspace(0.1, 0.5, 5, endpoint=True)  \n",
    "}\n",
    "\n",
    "Grid_RF = RandomizedSearchCV(estimator = Pipeline_RF, param_distributions = Param_RF, scoring = 'accuracy', cv = 5, n_iter = 100, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores Parametros: {'RF__min_samples_split': 0.2, 'RF__min_samples_leaf': 0.5, 'RF__max_features': None, 'RF__max_depth': 16.0}\n",
      "Mejor AUC en Train: 0.592\n"
     ]
    }
   ],
   "source": [
    "Grid_RF.fit(x_train_vect,y_train)\n",
    "print('Mejores Parametros: %s' % Grid_RF.best_params_)\n",
    "print('Mejor AUC en Train: %.3f' % Grid_RF.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resulatados del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión del modelo Random Forest con TF- IDF  calibrado es: 0.59\n",
      "El AUC del modelo Random Forest con TF- IDF calibrado es: 0.50\n"
     ]
    }
   ],
   "source": [
    "y_pred = Grid_RF.best_estimator_.predict(x_test_vect)\n",
    "y_pred_proba = Grid_RF.best_estimator_.predict_proba(x_test_vect)[:,1]\n",
    "print('La precisión del modelo Random Forest con TF- IDF  calibrado es:', '{:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('El AUC del modelo Random Forest con TF- IDF calibrado es:', '{:.2f}'.format(roc_auc_score(y_test, y_pred_proba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Modelo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Modelo 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
